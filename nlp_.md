[TOC]




## ·Word2vec & NLP  自然语言处理


#### 0.概述：文本分析 & NLP应用场景


​		·拼写检查

​		·关键词搜索

​		·文本挖掘：关键信息挖掘，自动摘要

​		·文本分类

​		·机器翻译

​		·客服、复杂对话、智能对话系统

​		"deep learning for nature language processing"

​			=》深入学习自然语言处理

​			=》自然语言处理的深度学习

​		使用深度学习的原因：

​			·手工特征费时费力难拓展 ==》自动特征速度快易拓展

​			·通用框架，广泛适用

​			·可监督学习，也可无监督学习


#### 1.N-gram语言模型


> 基于条件概率的N-gram模型


​	S = 我 今天 下午 打 篮球

​	=》p(w1) p(w2) p(w3) p(w4) p(w5)

​	=》p(S) = p(w1, w2, ...,wn)

​              		= p(w1)p(w2|w1)p(w3|w1,w2)...p(wn|w1,w2,...,wn-1)

​	其中 p(wi|w1,w2,...,wi-1) = p(w1,w2,...,wi-1,wi) / p(w1,w2,...,wi-1)

​	分析：当S很长时，数据过于稀疏(有些词组合的概率很小)，参数空间太大，p(S)计算过于复杂


​	N-gram模型， N = 1 时：

​		即假设下一个词的出现，只与前一个词相关

​			p(S) = p(w1) p(w2|w1) p(w3|w2) ... p(wn|wn-1)

​	N-gram模型， N = 2 时：

 		即假设下一个词的出现，只与前两个词相关

​			p(S) = p(w1) p(w2|w1) p(w3|w1,w2) ... p(wn|wn-2,wn-1)


​	=》假设语料库 (词典) 大小是 D，则n-gram模型参数的量级是 O(D^n)

​		n-gram的 n = 模型参数数量

​		1 (unigram)    2x10^5

​		2 (bigram)      4x10^10

​		3 (trigram)      8x10^15

​		4 (4-gram)     16x10^20


#### 2.词向量 【word2vec】


> 基于给定语料库


​	词库量为d的语料库 D:

​		D: [w_1, w_2, w_3, ..., w_d]

​		vec-w_1 = [1, 0, 0,...,0]

​		vec-w_i  = [0,..., 1,...,0]

​	长度为n的语句1	  S1 = [w11, w12, ..., w1n]

​	长度为m的语句2 	S2 = [w21, w22, ..., w2m]


​	·构造词向量模型:

​		输入格式：txtIn = [['my','cat',...,'cute'], ['today','is',...,'sunny'],...]

​		model = word2vec.Word2Vec(txtIn, 参数列表)：

​			min_count = ？	过滤掉词频低于？的词【1~100】

​			Size = ？		 设置神经网络层数，w2v默认100层【10~数百】

​		model.similarity('词1', '词2')  #  判断两个词向量 的相似程度


​	·python工具库

​		jieba: 分词器

​		opencc：繁体字 =转=》简体字


#### 3.神经网络模型 文本分析：


> 通常为使用RNN的 序列预测问题


​	假设有词库量为D的语料库，一个有 n 个词的语句 S(w_1,w_2,...,w_n)，给定 前n-1 个词w_1~w_n-1，利用神经网络模型 预测 第n 个词w_n是什么？

​		1.包括 前n-1 个词 各自的词向量 

​		2.输入语句样本 X：将这些词的向量拼接切来为 (n-1)*D 的矩阵(大向量)

​		3.输出层 y：yw = (y_w1, y_w2,...,y_wn)t

​			归一化：y_w1 + ... y_wn = 1 

​			=》y_wi 相当于第n个词是 语料库中第i个词 的概率

​		则 yw 中最大(概率)值对应的词，即该模型对于 该语句S的 第n个词的 预测结果


​	N-gram模型 与 神经网络模型 比较：

​		相对而言，N-gram模型过于依赖 历史数据

​		理想情况下，神经网络模型更符合 人类的语义逻辑


#### 4.Hierarchical Softmax：层级分类器


​	**CBOW模型**：【基于上下文环境的 预测(中间)词】

​		Continuous Bag-of-Words Model，根据上下文词语预测当前词语的出现概率的模型

​			L = ∑_ω{ log(p(ω|Context(ω))) }

​		输入 w(t-2) w(t-1) __ w(t+1) w(t+2)，经过隐层的运算，输出 w(t)


​		根据 语料库 词频统计（各词的权值）构造 【哈弗曼树】:	

​		哈弗曼树：带权路径长度最短二叉树，最优二叉树 =》权值 x 步长  之和最小

​			若将树的叶子结点为各个分词，权值为词频，分支节点(非叶子节点) 处必为左右2分类

​			使用logistic回归中的sigmoid函数: Sig(x) = 1 / (1+e^(-θtx)), 输入为(-∞，+∞)，输出为[0,1] 用于二分决策

​		给出以下预定义：		

​		pʷ 	【路径】	从根节点出发到达对应叶子结点w的 路径

​		lʷ 	【节点数】	路径中包含节点的 个数

​		p1ʷ,p2ʷ,...,plʷ 			【节点】	路径pʷ种的各个 节点

​		d2ʷ,d3ʷ,...,dlʷ ∈ {0,1}	    【词编码】    词w的编码,diʷ 表示路径pʷ第i个节点对应的 编码 (根节点无编码)

​		θ1ʷ,θ2ʷ,...,θ_(lʷ-1)ʷ ∈Rᵐ      【参量】	路径pʷ中分支节点对应的 参数向量


​		·利用梯度上升、对数似然法，求解参数更新法则

​		·负采样：区分正样本和负样本，0,1随机法 随机抽取 一部分负样本来参与 模型更新

​	

​	**Skip-gram模型**：【基于(中间)词预测其上下文环境】

​		输入 w(t)，经过隐层的运算，输出 w(t-2) w(t-1) __ w(t+1) w(t+2)


#### 5.文本挖掘技术：


​	**1.停用词：**

​		·跟新闻类别没有太大关系，却可能在特征类别的新闻中经常出现的 词汇

​		·特点：预料中大量出现，与文本主题内容没什么关系

​		·资源：网上可以找到现成的 停用词表，通常包含各种标点字符，以及常见量词代词语气词等


​	**2.Tf-idf  关键词提取：**

​		TF：词频(Term Frequency 简写为TF)统计

​			如在《中国蜜蜂养殖》中，出现次数较多的

​			·如“的”“是”“在”...为停用词

​			·如“中国”“蜜蜂”“养殖”出现次数 与 重要性 =》 

​			相对而言，“中国”是很常见的词，重要性较低


​		IDF：逆文档评率(Inverse Document Frequency)

​			如果某个词平时(在预料库中)比较少见，但在这篇文章中多次出现，它可能是我们需要的关键词


​		TF 词频  = 某词在该文章中出现次数 / 该文章中出现次数最多的词的出现次数

​			或者 = 某个词在文章中的出现次数 / 该文章的总词数

​		IDF 逆文档频率  = log( 语料库中文档总数 / 包含该词的文档数 + 1 )

​		TF-IDF = 词频(TF) x 逆文档频率(IDF)


​	**3.相似度计算：**

​		·文本

​			A：我喜欢看电视，不喜欢看电影。

​			B：我不喜欢看电视，也不喜欢看电影。

​		·分词

​			A：我/喜欢/看/电视，不/喜欢/看/电影。

​			B：我/不/喜欢/看/电视，也/不/喜欢/看/电影。

​		·语料库

​			[我，喜欢，看，电视，电影，不，也]

​		·词频统计（基于语料库的 出现次数）

​			A:[我1，喜欢2，看2，电视1，电影1，不1，也0]

​			B:[我1，喜欢2，看2，电视1，电影1，不2，也1]

​		·词频向量

​			A[1, 2, 2, 1, 1, 1, 0]

​			B[1, 2, 2, 1, 1, 2, 1]

​		·相似度(n为语料库长度)

​			cosθ = cos<A,B> = ∑_n{ Ai x Bi } / √∑_n{ Ai² } x √∑_n{ Bi² }

​						    = A·B / |A|·|B|

​						    = 13 /  √12 x √16

​						    = 0.938


#### 6.Bag of words 词袋模型：


​	将一个文本看作 分词的集合，而忽略词序与语法。


#### 7.LSI / LSA 隐语义索引 / 分析、pLSA 概率隐性语义分析 


​	通过矩阵SVD分解，提取潜在的不可解释的信息表示。该模型存在的问题

​	SVD计算非常的耗时：非负矩阵分解（NMF）

​	选取较优参数困难：（主题个数k）一般是经验选取，也可利用层次狄利克雷过程（HDP）自动选取

​	缺乏统计基础和概率上的直观解释：提出 pLSA 或 LDA 模型


​	pLSA 一般采用 EM算法 来求解模型参数


#### 9.LDA 主题模型：（隐狄利克雷分布）


​	gamma 函数 - Dirichlet 分布、（类似的有 二项分布、多项分布、beta分布）

​	贝叶斯框架 & 共轭先验、

​	Gibbs 采样




10.NLP软件


​	HTK软件 - 自然语言处理·HMM、openNlp、IBM-Watson API


11.经典书目：《宗成庆-统计自然语言处理》《统计自然语言处理基础》《自然语言处理综论》


12.NLP技术发展阶段


​	·以语言学为主要基础：一句语言规则使用数理逻辑进行推理

​	·以统计方法为主

​	·深度学习在NLP中的应用


13NLP研究内容


​	信息检索、机器翻译、文档分类、信息过滤、信息抽取

​	智能问答、自动文摘、舆情分析、机器写作、语音识别

------


#### 注意力机制  Self-Attention：


​	Attention函数的本质可以被描述为一个查询（query）到一系列（键key-值value）对的映射。

​	如图![1557150189585](C:\Users\Tinkle_GW\AppData\Roaming\Typora\typora-user-images\1557150189585.png)


​	在计算attention时主要分为三步：

​		第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；

​		第二步一般是使用一个softmax函数对这些权重进行归一化；

​		最后将权重和相应的键值value进行加权求和得到最后的attention。目前在NLP研究中，常常都是key=value。


这一步相当于一种激活方法，将多个输出值归一化为一个统一的输出值，按照注意力机制分配权重，传递给下一步计算

------




## ·时序数据分析


#### 0.概述：时间序列数据要素


​		·时间戳

​		·固定周期（总时序采样次长）

​		·时间间隔（没次采样间隔）


​	ARIMA模型：AR, I, MA

​	


#### 1.时序数据特征


​	·平稳性：由时序类样本数据所拟合的曲线，在未来一段时间内呈现出一定的“惯性延续”形态。平稳性要求 序列的均值、方差不发生显著变化

​		1.严平稳 & 弱平稳

​			严平稳：假设序列数据严格服从 正态噪声（白噪声），无论取哪一段序列的数据，都是期望为0，方差为1

​			弱平稳：期望E 与 相关系数ρ 不变

​				=》序列中出现的数值，依赖于过去的信息（对历史数据有一定的依赖性）

​		· 一般认为只有理想情况下才有 严平稳的时序数据

​		· 大多情况下为 弱平稳 的书序数据


#### 2.差分法


​	一阶差分：时间序列在 t 与 t-1 时刻的差值

​	二阶差分：对一阶差分序列数据 再做一阶差分

​	···


#### 3.自回归 （AR模型）：用历史时间数据预测当前数据


​	p阶 自回归过程：（阶 指的是 差分阶数）

​		公式 yt = μ + ∑p_i{ γi * yt-1 } + εt

​		yt预测值   μ常数项   p差分阶数   γi自相关系数  εt误差

​	AR模型的局限性：

​		·必须满足平稳性要求

​		·只能是自身数据来进行预测 (预测与自身前期相关的现象/规律)

​		·必须具有自相关性 (若自相关系数 φi < 0.5 ,不宜使用)

​	


#### 4.移动平均（MA模型）：MA关注AR模型中的误差项的累加，能有效地消除 预测中的随机波动


​	q阶自回归过程：

​		公式 yt = μ + ∑q_i{ θi * εt-1 } + εt


#### 5.ARMA 自回归移动平均模型


​	公式 yt = μ + εt + ∑q_i{ θi * εt-1 } +  ∑q_i{ θi * εt-1 }

​	【三个参数：差分阶数p，d时间序列为平稳时所做的查分次数，q移动平均项数】


#### 6.ARIMA 差分自回归移动平均模型：全称 Autoregression Integrated Moving Average Model


​	·AR 自回归，p 自回归项-差分阶数，

​	·MA 移动平均，q 移动平均项数，

​	·d 时序平稳期 所做差分次数


​	原理：将非平稳期序列 转化为 平稳期序列，然后将 因变量 仅对

​		其滞后值(阶数) 以及 随机误差项的 值 和误差滞后值 进行回归所建立的模型


#### 7.pq值选择问题：


##### 	自相关函数 ACF (Autocorrelation Function)：


​		有序的随机变量序列 与其自身相比较，反映了 同一序列 在不同时序的取值 之间的相关性

​		公式：ACF(k) = ρk = Cov(yt, yt-k) / Var(yt)	=》滞后k自相关系数 ρk

​			其中Cov()求协方差，Var()求方差，ρk∈[-1,1]，yt 为原时序数据列，yt-k 为原序列的k阶差分序列

​		


##### 	偏自相关函数 PACF (Partial Autocorrelation Function):


​		对于一个平稳的AR(p)模型，求出的 滞后k自相关系数ρ(k)，实际上并不是单纯的 yt 与 yt-k 之间的相关关系

​		也就是说，本质上 yt 与 yt-1, yt-2, ..., yt-k-1, yt-k 都具有相关关系，ρ(k) 里面掺杂了其它变量序列 对 yt 与 yt-k 的影响

​		=》	ACF还包含了 其它变量的影响

​		=》	PACF 则是 (剔除了其它随机变量yt-1~yt-k-1的干扰后) 严格的 两个变量 (yt 与 yt-k) 之间的相关性

​		

​	python statsmodels 库具有计算 ACF & PACF 的API


​			ACF模型			PACF模型

​	AR(p)		 衰减趋于零			 p阶后截尾

​	MA(q)		 q阶后截尾			  衰减趋于零

​	ARMA(p, q)	q阶后衰减趋于零	       p阶后衰减趋于零

​		【截尾：落在置信区间内】

​		【置信区间：95% 的点都处于置信区间内部】

​	=》p 看PACF（哪一阶开始稳定在置信区间内|截尾）,q 看ACF（哪一阶开始稳定在置信区间内|截尾）


#### 8.ARIMA 建模流程：


​	1.将序列 平稳（平稳化）（用差分法 确定d）

​	2.p，q阶数确定： ACF和PACF

​	3.ARIMA(p, d, q)


#### 9.参数选取准则：AIC 和 BIC


​	【k为模型参数个数， n为样本数量，L为似然函数】

​	【值都是越低越好 =》倾向于选择更简单的模型】

​	AIC：赤池信息准则 Akaike Information Criterion

​		AIC = 2k - 2ln(L)


​	BIC：贝叶斯信息准则 Bayesian Information Criterion

​		BIC = k*ln(n) - 2ln(L)


#### 10.模型诊断：残差检验 与 QQ图




