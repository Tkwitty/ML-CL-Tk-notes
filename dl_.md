[TOC]



------


#### 图像处理：


​	傅里叶变换

​		像素的频域分析，一个图像的频谱图可以分解成无限个正余弦波的叠加

​		目的是可以很好地反应出图像的周期性变化，从而对图像的周期性噪声做降噪处理

​		傅里叶变换 =》 频域图像

​		空域，时域，频域

​	图像 · 距

​	色彩空间：HSV---色调，饱和度，亮度	RGB---红绿蓝

​	图像位图深度 8/16/32

​	各种像素点值类型  uint

​	一阶梯度&二阶变化率

​	仿射变换&透视变换 

------


人机交互 -> 信息传递 -> 信息载体 -> 

	电磁波： wifi, 无线网络，蓝牙【计算设备之间的 数据通信】	
	
		可见光：光学成像，视觉信息【从物理世界采集信息到 计算机图像数据】	
	
	机械波:   声音【从物理世界采集声波信息到 计算机音频数据】


​	脑电波，是生物电流，大脑在活动时，脑皮质细胞群之间形成电位差，从而在大脑皮质的细胞外产生电流。

​	脑电波同步节律的形成与皮层丘脑非特异性投射系统的活动有关，“脑电波接收器”，可以把脑电波转化为普通电波。

------

------


#### 神经网络 & 梯度下降：


​	(权重，偏执) => 激活 => 神经元输出值

​	样本训练之前  (会随机的初始化所有的 权重&偏执 ， 对于一个样本的输入【高维向量】，输出一个 包括所有标记类型对于的 输出值

​	（这个输出值理解为  神经网络 对于当前的输入值，估计它的标记 是这个标记的 概率）

​	因为这个样本的标记已知，取一个标记容量维数的向量，在已知标记对应的值取1，其它全为0。

​	于是，将这个向量  与  神经网络的输出向量  的各项差值的平方和。

​	这个和值  即当前神经网络 模型的 性能好坏的衡量 ，注意到，这个值越大，说明模型越糟糕， 越小说明模型越好。 于是这个   差值平方和  可称之为 模型的 代价loss。


​	一个样本对应一个loss值，那么一个超大样本集对应同数量的loss值集，这个loss值集的 均值， 即相当于  这个【已训练好的模型】好与坏的评价标准（而且这个值越小越好）


​	于是，神经网络模型的 一次训练 可以看作是一个这样的函数 =》

【系数：整个模型所有权重&偏执的函数 F（自变量：对于某一个样本输入向量&标记向量）=  因变量：这个样本下得到的代价值LOSS】----》


​	于是, 理想的目标是使得这个模型, 在经过超多次数的训练  (即调整系数：所有的权值&偏重)  过程 之后，能够使得 在某个超大样本集的输入下，其代价loss的均值(期望) 最小。


​	这又是一层函数：【系数：一个超大训练样本的输入向量和标记的函数 G（自变量：模型中的所有权重&偏执）= 因变量：模型的代价值Loss的期望】

​	于是不难理解，我们  训练的目标就是 求出是这个  极其复杂的函数（损失函数） 在loss期望(因变量)取得最小值时，它的各项系数(自变量)是多少？




​	G（x）= y		y在取最小值时，x应取多少的问题  ===> 


​	如果训练过程中的某一个时刻的模型本身  对应于 落在这个G函数图像 (超平面) 中的某一个点，


​	则【梯度下降】 中的梯度即  G函数本身 在这个点上的 导数值 =》这个值是一个维数为【所有权值&偏执的总数量】的向量，

​	不难理解 当函数图像中的一点 沿着该点的梯度向量的 反方向移动，一定是 朝着函数的某个极小值点方向移动   (这个极小值可能是 局部最小值或者全局最小值)

​	因此，我们 对这个导数值（向量）取反， 则应该是  原因变量【所有权值&偏执】应该移动的方向

那么  对这个导数值向量取反， 并乘以一个预先设定学习率【可理解为 对每一次训练所给与的 点在G超平面中移动的步幅大小】，


​	于是将 原自变量【所有权值&偏执】的向量   与  这个导数值取反和学习率的乘积得到的向量   相加求和 得到一个新的  自变量。（即对应与G图像上的一个 新的点）


​	这就是  模型 在经过了一次样本训练 的过程中的 “学习” 的过程。


​	[随机梯度下降算法](SGD) 中，随机指的是，随机均匀的小批量抽样，梯度指的是代价函数L对L的导数(向量)，下降指使代价值期望趋于最小。

​	其思想是，若对于每一个样本的代价计算之后都要进行一次代价函数的求导和更新，会耗费巨大的计算资源，

​	为此采用随机采样方法，即可以节约计算资源，又能保证不亚于逐次更新的情况下的效果。因为在样本足够大时，随机小批量采用具有一定的代表性。

​	随机梯度下降的其他变种：

​		Hessian优化、Momemtum-based gradient descent


​	除了Sigmoid函数之外的【激活函数】：

​	tanh(wx+b)、rectified linear 神经元 max(0,wx+b)、、、激活函数的更好的选取还在研究中


​	a是激活值， b是偏执值，w是权重值


===============


#### BP算法的四个重要公式推导：


​	·交叉熵-作用： 

​		因为w&b的初始值是随机的，在相同的训练迭代次数的情况下，不同的初始值可能会导致模型的学习效果差别很大。比如当初始值与期望值相差很大时，导致在整个训练过程的初期所有w&b更新幅度很低，即学习缓慢。通过公式推到分析了解到，导致这个的重要原因是  sigmoid  激活函数 的特性：在值接近0/1的时候，梯度相对平缓。


​	·softmax函数：在最后一层隐藏层到输出层时使用，目的是使得在输出层的每个输出单元的值的总和为1，因此每个输出值可以看做概率p。


​	·Batch-size: 批尺寸/批大小, SGD中每次随机抽取的小批量样本的数量

​	·Iteration: 一个batch-size的样本量 训练一次的过程

​	·Epoch: 用训练集的全部样本进行一次完整的模型训练的过程 【一般用来定义模型迭代训练的终止条件】


#### Overfitting 过拟合问题：


​	在训练集表现好，但不能泛化到测试集或将模型投入到外部应用场景（也就是表现不好）。【或 测试集&训练集上的代价||准确率变化过程差距较大】

​	过拟合深层原因，一般是因为训练集本身不能覆盖样本所处框架下的足够的占比空间；

​	在有限的样本表现下，训练出了过拟合的模型参数，使得该模型在真正应用到广泛场景时 的表现往往比模型训练或测试时的表现要差。


​	在神经网络中，拟合性本身可以用  w&b系数  的性质做参考，一个直观的参考则是 w&b 系数的数量。

而对于w&b数量最直接的影响因素就是 神经网络模型 的层数和各层的神经元数了。	


​	所以，如果在样本所属框架下，对于样本分类问题本身的复杂性，应该跟模型对于分类问题的拟合性  呈正相关。

也就是当 分类问题本身 就比较复杂时，应该给予一个相对复杂的神经网络来拟合真实分类过程，同样的对于简单分类问题给予简单模型。


​	所以造成过拟合情况的参考量有： 分类问题的复杂性、 模型的复杂性、 样本量对于样本类别空间和样本框架覆盖程度【样本量】

​	1.【问题复杂，模型简单】	当问题复杂，模型简单时，已经失去意义，样本量可不参考

​	2.【问题简单，模型复杂】	当问题简单时，大样本在所有类别空间密集覆盖，后期训练样本价值流失

​	3.【模型复杂，样本量少】 	

​	解决方案：根据分类问题复杂性构建合适的模型；有足够多的样本，理想情况时样本集在各个类别空间中都均匀覆盖（每个类别都覆盖，各类内按真实概率分布而分布）




#### 正则化 Regularization：  


​	（在给定的神经网络，给定的训练集的情况下）一种新的减少 overfitting 的解决方案！

​	通过调整 C 的增加项中的 lambda 系数，可以人为调整 模型学习过程的倾向性

​	【lambda值越小，增加项的值趋于0，正则化不会对模型学习过程产生影响。lambda值越大，总体权值在学习过程中倾向于变小】

​	由于正则化Regularization网络 鼓励更小的权重，使得样本x的一些随机变化甚至极端状况（样本本身具有噪声）不会对总体模型造成太大影响【抗噪声特性】

​	正则化网络 倾向于拟合为 更简单的 模型，简单模型不一定总是好的，但是大量实验证明正则化后可以使模型更好的泛化更多实验中，理论支持还在研究中……

​	


#### Dropout：


​	不同的模型训练相同的数据集，可能会得到不一样的结果。

​	为了避免这种情况，在一次mini-batch的正向反向更新wb的周期后，随机的删除掉一定比例的神经元，再进行下一次的正向反向更新，如此循环下去。

​	因为在固定结构的模型中，某些神经元很可能严重依赖于一些特定的神经元，通过随机删除的方法随机改变模型结构，可以使模型学习结果又更强的泛化能力。


#### 权重初始化：


​	从均值为0，方差为1的正态分布（均值0方差1）中产生权重值。  

​	==》 弊端：输出值激活前不是过大就是过小，导致sigmoid激活后不是接近于1，就是接近于0，于是权重变化时，更新量很小，学习慢

​	改进的权重初始化方法：	从均值为0，标准差为 1/sqrt(n_in)     ->输入层神经元个数的开方的倒数 

​	==》 改进后可使得绝大多数输出值在0到1之间

​	【注意：改进的初始化方法，会加快学习速率，对于有些神经网络最终表现是一样的，也有些神经网络中会提高最终的accuracy】


#### Hyper-parameters 优化 ：	


​	---超参数即除w&b之外的参数如：学习率、正则化lambda的设置：

​	当学习率、正则化lambda参数都设置到非常大时，会导致神经网络表现非常糟糕（差到跟随机猜一样）。。

​	神经网络可变因素总结：结构（层数&神经元数)，wb初始化方法，cost函数、正则化&参数、sigmoid/softmax、Dropout、训练集大小、batch-size、学习率


#### 参数调优：	


​	学习终止条件：一段时间内变化很小（阈值）)

​	1.学习率：从0.001,0.01,0.1,1,10...尝试，若发现cost开始增大，则回调。

​	2.先不使用正则化，把学习率调整好，然后在开始增加正则化并调节lambda参数

​		从1,10,100...尝试，若发现表现变差，在回调。

​	3.batch-size:	太小没有充分利用矩阵计算lib和硬件整合条件	太大则更新w&b不够频繁

​	*.grid search 【网格搜索】：一种自动搜索各种参数组合的方法

​	*.还有一种调优方法：Ensemble of network: 训练多个神经网络，投票决定结果

​	*.利用进化算法，求解近似的全局最优超参数组合


​	===》调参是一个正在研究的课题...



​		


#### 深度神经网络 的 不稳定性和难点：			


###### 	·Vanishing Gradient 梯度消失问题：


​		当加深神经网络层数后普遍遇到的一个问题：不同层的学习速率显著不同，前面的层学习和更新速率太慢，而接近输出层的学习速率大很多。

​		因为前馈神经网络在反向更新的时候，更新值会从输出层开始逐层递减，层数太深导致前面层的w&b的值趋于不变。


###### 	·Exploding Gradient 梯度爆炸问题：


​		由于考虑到梯度消失问题，尝试调节b的值，使得z等于0，但在这种情况下每层的偏导数又是前一层的很多倍，导致前面层的更新幅度反而远高于后层，即出现了梯度爆炸问题


​	解决方案：使用  ReLU函数  取代 Sigmoid激活函数，因为sigmoid函数的导数在x=0是达到最大1/4，函数梯度会随着x的增大或减少而消失，而且relu函数的导数在x>0情况下是一个常数， 梯度不会随着x的变化而消失




#### 卷积神经网络 Convolution Neural Network:


​	权重=》	卷积核：一种卷积核的体量（对一个图像样本的每一个数值都做了卷积操作）， 卷积核的种数即是后层神经元的个数

​	偏执=》	池化核


​	池化核（降采样），在其尺寸和步长与上一次卷积的结果大小不能全局覆盖时，直接丢弃边缘维度值，不是丢弃而是 补0，来完成完整的池化过程


​	全连接层，还是一次卷积过程（没有池化，因为已经是最低维1了），这次的卷积核尺寸就是全连接层的输入值的尺寸，输出一个值

若全连接层后面还有全连接层，则这次卷积核尺寸1x1，而卷积核数量是后层全连接层数量，输出为1x1x全连接层大小


​	CNN 只对最后一层(全连接？)用 dropout，因为CNN本身的卷积过程对于 overfitting 有防止作用：共享权重使得卷积核强迫对整个图像进行学习（特征感知）


​	CNN相较传统神经网络的优势：

​		大大减少了参数数量、可以利用GPU加速计算，每次更新很少但可以训练很多次。

​		目前最多的神经网络层数 20 层。

------


#### 自组织神经网络 SOM：


​	Self-Organizing Feature Map/Kohonen映射，简称SOM网络，用于解决无监督聚类问题

​	SOM不需要指定聚类数量，类别数由网络自动识别，原理是将距离小的个体集合划分为同一类。

​	SOM只有两层：

​		输入层---样本的维度即输入单元的数量

​		输出层/竞争层---每个输入单元，与所有的输出层单元都有连接，而输出层的单元之间也有侧向连接，通过对连接的权重参数学习，形成特定的模式。




#### 玻尔兹曼机 Boltzmann Machine ：


​	非监督学习模拟了金属加热退火过程的三个阶段：

​		1.加温，增强粒子间的热能使之能够自由移动

​		2.等温，保持环境温度使系统不与外部发生热交换，粒子自发朝自由能减少的方向变化，自由能最小时系统平衡

​		3.冷却，系统处于平衡态时，粒子渐趋有序会构成特定排列，冷却后仍处平衡态，内能最小

​	网络使用了玻尔兹曼能量函数而得名，利用了统计力学/数学中的 Boltzmann分布 / Gibbs分布 作为网络的激活函数

​	优势：能够跳出函数的局部最值，通过足够的迭代次数可找到近似的全局最值，常用于求解路径最优方案，应用广泛

​		


#### 限制玻尔兹曼机 Restricted Boltzmann Machine:


​	由于 **玻尔兹曼机** 是一种 非监督学习，也就是对所有训练样本都不知道其对应标签。

​	相对于传统神经网络，正向传输还是一样通过： 权值、偏执、求和、激活、逐层向后传输知道最后一层隐藏层时，将隐藏层当做输入层反向更新，使用原来的权重和新的Bias。【双向全连接】

​	反向传输直到原输入层时得到的激活值a，与原输入值x必有一定差值error，再通过建立一个使得这个error最小化的方程来迭代地更新后面的参数。

​	1.正向更新时：用输入x 预测神经元的输出值为a发生的概率		

​		【在给定权重情况下，对于输入x，输出为a的概率】

​	2.反向更新时：用输出a 预测原始的数据x发生的概率		

​		【在给定权重情况下，对于给定的a，输入为x的概率】

​	3.结合12，模拟x和a的 joint probability distribution 联合概率分布： p(x,a)


Generative learning: 模拟输入数据x的概率分布

Discriminative learning：把输入映射到输出，区分几类点


#### 深度信念网络 Deep Brief Network:


​	一种神经网络类型，由多个限制玻尔兹曼机 Restricted Boltzmann Machine 组成的神经网络。


#### 深度自编码器： 


​	由两个对称的 DBN深度信念网络 组成，靠近输入端的DBN作为 编码器，靠近输出端的DBN作为 解码器，正中间是由编码器处理得到的 压缩特征向量。

​	作用： 降低维度，图像搜索，数据压缩，信息检索


#### SC 稀疏编码：


​	一种无监督学习方法，用来寻找一组**“超完备”**基向量来更高效地表示样本数据。稀疏编码算法的目的就是找到一组**基向量**，使得我们能将输入向量 X 表示为这些基向量的线性组合。

​	超完备：假设x为Rn维特征，则基向量的维度k > n，好处是能有效地找出潜在的数据结构与模式。

​	稀疏：表现在于，用基向量表达每个样本x时，尽可能使加权系数 ai 只有少数几个不为零(或远大于0)。 


#### SAE 稀疏自编码：


​	

------


#### RNN 递归神经网络：


​	输入为时序数据，建立序列关系 (注意：递归网络 和 循环网络 不属于一类)

​	最基本原理：每条数据的在经过一层单元后不仅仅向后层传递，还可以在本层循环（前面的数据产生中间信息对后面传进来的数据产生影响）

​	神经单元过长，对于预测结果来说，前面的数据可能对目标结果没有什么影响，但是依然参与了运算

​	RNN面临的问题：

​		链式模型导致更严重的梯度消失现象，计算量太大，不必要的信息太多

​		【改进目标，有选择的忘记一些信息，利用好新近的一部分信息】


#### LSTM长短期记忆网络:


​	处理=》 离散(时间)序列数据（sequential data）：离散线性，长度可变

​	应用=》 序列数据的 分析-趋势预测、生成-基于图片的诗歌创作、转换-语音识别&机器翻译

​	相较于 RNN，引入了 

​		·门：一种让信息选择式通过的方法

​			sigmoid神经网络层 & 一乘法

​			sigmoid层 输出0,1之间的数值，这个输出可作为 控制参数C 


​		·控制参数 C：

​			决定要保留多大权重的遗留信息。（0,1）的权重值

​			如何训练C参数？		

​	

​		·ft = σ(Wf·[h_t-1,xt] + bf) 			【此处激活函数 ft的输出值 取值范围（0,1）】

​			ft & C_t-1 决定丢弃什么信息		【相乘 ft * C_t-1】


​		·it = σ(Wi·[h_t-1,xt] + bi)				【it 为要保留下来的信息】

​		·Ct= tanh( Wc·[h_t-1,xt] + bc )	【Ct 为新数据形成的控制参数】 

​		·Ct = ft * C_t-1 + it * Ct`				【决定下一单元的控制参数 Ct】

​		·ot = σ(Wo·[h_t-1,xt] + bo)

​		·ht = ot * tanh(Ct)					【利用新的控制参数 产生输出】


​	过程：

​		输入-》遗忘门-》保留门-》输出门	（ · -》遗忘参数-》保留参数-》数据流-》· ）

------


#### 迁移学习、图卷积、自主学习

------


