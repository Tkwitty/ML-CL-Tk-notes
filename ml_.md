**机器学习算法**


[TOC]




### 0.回归【线性回归 ， 逻辑回归 ， 非线性回归】


​	0.概述：是研究一组随机变量y（因变量）和另一组随机变量x（自变量）之间关系的统计分析方法。

​		回归误差：

​			y真实 = y预测 + ε误差	

​		误差的高斯分布：

​			未说明情况下，一般认为案例所给的有限样本中，对于每个样本的 真实标记值y 与 其根据回归函数所得的预测值 之间的误差

​			e(i) 是服从均值0，方差δ^2的高斯分布（正态分布）

​		似然函数：

​			样本标记y，样本数据x，未知的回归参数w。构造函数 L(w) ，要求对于整个样本集的每个数据x，使得标记集y 出现的概率最大

​			可以令 L(w) =【所有x可能的取值为条件时，对应得标记y出现的概率】的连乘， 求当L函数取得最大值时w为何值时？

​			求解 当所有样本的 后验概率 的连乘值 最大时，求验参数的值


​		先验概率：事情还没发生，求这件事情发生的可能性大小。

​		后验概率：事情已经发生，求这件事情发生的原因是由某个因素引起的可能性大小。

​	

​		对数似然法：

​			即似然函数等式左右取自然对数，由于log()/ln()不会改变原函数的单调性，而对于以求极值为条件的问题，可以将原函数转化为原函数的对数函数，以简化运算

​		最小二乘法：

​			通过最小化“误差的平方和”来寻找数据的最佳拟合函数。（在给定回归函数的情况下，使得x观测值集的对应的y预测值集与y真实值集直接误差的平方和最小）

​		正则化：

​			当求参数w时可以求得多组解的情况下，可通过算法的归纳偏好决定取哪组解，此时可引入正则化项。

​		广义线性模型：

​			原回归模型为 y = wx + b，若单独对因变量 y 添加一个【联系函数】的映射关系g(.)【单调可微函数】，也可进行模型学习

​			例如构建 【对数线性回归】模型 lny = wx + b 

​		梯度下降 & 梯度上升: 

​			在学习算法过程中，往往通过构建xx似然函数J(θ)或其转化函数JT(θ)，求其最值的问题，一般的，

​			求最大值时，用梯度上升法

​			求最小值时，用梯度下降法

​		参数更新：

​			（1）找到当前最合适的方向 =》偏导数值

​			（2）朝着这个方向走一小步 =》学习率控制步长

​			（3）按照方向和步长更新参数

​		凸优化理论:

​			在凸优化中局部最优值必定是全局最优值

​			求凹函数f最大值的问题  等同于  求凹函数-f最小值的问题

​		常见优化方法介绍:	【梯度下降法、牛顿法、拟牛顿法、共轭梯度法】

​			梯度下降法：当目标函数是凸函数时（凹凸函数 =》凸函数），局部最值解 即是 全局最值解。

​			牛顿法：一种在实数域和复数域上近似求解方程的方法，使用函数f(x)的泰勒级数的前面几项来寻找方程f(y)=0的根。优：收敛速度快

​			拟牛顿法：由于牛顿法每次需求解Hessian矩阵的逆矩阵，它使用正定矩阵来近似Hessian矩阵的逆，简化了运算的复杂度。

​			共轭梯度法：利用一阶导数信息，克服了梯度下降法收敛慢的缺点，避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点


#### 1.线性回归：


​	对自变量和因变量之间建立 线性关系，基本数学模型：

​		y = w1x + w2x + ... + wix + b 【=》y预测 = Σθx(i)】	【1.1】

​	由回归误差 =》 ε误差 = y真实 - y预测 =》 ε(i) = y(i) - Σθx(i)	【1.2】

​	有误差的高斯分布ε(i)~N(0,δ^2)： p(ε(i)) = (√(2π)*δ)^(-1)exp(-(2δ^2)^-2)*ε(i)^2)	【1.3】

​	【1.2】代入【1.3】有：p(ε(i)) = p[y(i) - Σθx(i)] =  (√(2π)*δ)^(-1)exp(-(2δ^2)^-2)*ε[y(i) - Σθx(i)]^2)	【1.4】

​	【1.4】中的 p[y(i) - Σθx(i)] 可写为条件概率形式：p(y(i)|x(i);θ)

​	于是由 似然函数 有 L(θ) = ∏[p(ε(i))] = ∏[p(y(i)|x(i);θ)] = ... 

​	由对数似然法有 ln(L(θ)) = ... = n*ln(√(2π)*δ)^(-1)) -  δ^(-2)*(1/2)*Σ[y(i) - θx(i)]^2		【1.5】

​	【1.5】中，令 J(θ) = (1/2)*Σ[y(i) - θx(i)]^2 【1.6】，于是原需求改为求当J取得最大值时θ的值

​	所以有： J(θ) = (1/2)*Σ[θx(i) - y(i)]^2 = (1/2)*[(Xθ - y)^T]*(Xθ - y)	【1.7】	

​	【1.7】经过矩阵运算 & 矩阵求导得：(əJ/əθ) = (1/2)*(2XtXθ - Xty - Xty) 	【1.8】

​	令【1.8】=0 得，θ = (XtX)^(-1)Xty

​	于是学得的线性回归模型为：	y预测 = Xt · (XtX)^(-1)Xty


#### 2.Logistic regression (逻辑回归) 对数几率回归:	


​	(对数log 几率odds =》logit)

​	提出问题：如何使用回归学习的线性模型，来做分类任务？

​	根据广义线性模型的思想，找到一种联系函数【单调可微】，将线性回归模型的预测值 联系为 分类任务下的真实标记y

​	例如对一个二分类任务，令输出标记y∈{0,1}，若有原回归模型 z = wx + b,要将实值z转换为非0即1类型的值，首先想到“单位跃阶函数”（即分段函数）

​	使当实值z > 0时判为1（正例），<0时判为0（负例），但由于该函数的不连续性，不能作为 联系函数，此时考虑找到一种近似(形似)该函数的替代函数=》Sigmoid函数

​	其中，Sigmoid函数的重要代表 对数几率函数（logistic function，简称 对率函数）为:   y = (1+eap(-x))^(-1)	

​	将z=wx+b代入对率函数后，将wx+b提到等式一边可得： ln(y/(1-y)) = wx + b		【2.1】

​	

​	---》【以下是 周志华·机器学习 的推导, 对于【2.5】的推出存疑】

​	思考，若将y的值（非0即1）视为样本x 是否为正例的概率（即 y = p(x->正例)，若x是正例,则p(x->正例) = 1 = y；x是反例,则p(x->正例) = 0 = y) 

​	=>  y = p(x->正例)  => y = p(y=1|x)，而有 p(y=1|x) + p(y=0|x) = 1 (由于样本x的类别非正即负，概率和为1)

​	=> p(y=1|x) = e^z / (1+e^z)，其中z=wx+b。p(y=0|x) = 1 / (1+e^z)	【2.2】

​	此时通过  极大似然法·对数似然  来估计参数w、b (简便起见，将w,b统一写作θ)，有 ln(L(θ)) = ln∏(p(y|x;θ) = Σ[ln(p(y|x;θ))]		【2.3】

​	令  p(y=1|x;θ) = p1，则  p(y=0|x;Θ) = p0 = 1 - p1 

​	p(y|x;θ) = y*p1 + (1-y)*p0 (若y=1，p = p1；若y=0，p = p0，条件概率的推导) 	【2.4】

​	【2.4】代入【2.3】有：ln(θ) = Σ[ln(y*p1 + (1-y)*p0)]	

​	在代入【2.2】得到：ln(θ) = Σ(-yθtX + ln(1 + eap(θtX)))	【2.5】

​	

​	---》【以下是 tyd·机器学习经典算法 的推导】

​	设目标预测函数为： h(x) = g(θtX) = 1 / (1+exp(-θtX))		【2.6】

​	在分类任务中，令 p1 = p(y=1|x;θ) = h(x) 【将h(x)函数值视为样本x是否为正例的概率】

​	同理， p0 = p(y=0|x;θ) = 1 - h(x)  【由于样本非正即负，概率和为1】

​	整合p0、p1有：p(y|x;θ) = [h(x)^y] * {[1-h(x)]^(1-y)} 【对于这个条件概率，y=1时，p = p1；y=0时，p=p0】		【2.7】

​	此时通过  极大似然法·对数似然 有：ln(L(θ)) = ln∏(p(y|x;θ)）	【2.8】

​	【2.7】代入【2.8】有：ln(L(θ)) = ln∏(p(y|x;θ)）=  ln∏([h(x)^y] * {[1-h(x)]^(1-y)}) =  Σ[y*lnh(x) + (1-y)ln(1-h(x))]		【2.9】

​	因为此时的需求为 求当ln(L(θ))取得最大值时θ的值，应用 梯度上升求最大值

​	令 J(θ) = (-1/n) * ln(L(θ))，于是原需求改为求当J取得最小值时θ的值，应用 梯度下降法。

​	【注意，梯度上升 & 梯度下降 方法在Logistic回归中它们的思想是相同的，可以任意使用其中之一】

​	J(θ) = (-1/n) * Σ[y*lnh(x) + (1-y)ln(1-h(x))]， 其中 h(x) = 1 / (1+exp(-θtX)) = g(θtX)

​	由于求导目标为θ，改写为：J(θ) = (-1/n) * Σ[y*lng(θtX) + (1-y)ln(1-g(θtX))]

​	所以有 (对于θ向量中的某个参数θj求偏导) ： (əJ/əθj) = ... = (-1/n) * Σ[(g(θtX) - y)*xj]		【2.10】

​	由梯度下降思想&偏导数（数值）的定义可知：

​	对于参数θj的更新： θj := 原参数值θj - 更新步长(学习率) * 梯度方向(偏导数值)

​	=>  θj := θj - α * (əJ/əθj) = θj - α * (-1/n) * Σ[(g(θtX) - y)*xj]		【2.11】


​	---》二分类的 Logistic 回归 拓展到 多分类的 Softmax 时：（详见 神经网络算法·Softmax分类器）

​	相对于Logistic 回归，只需根据 y 的多种取值，构建使得 p(y|x;θ) = 各个分类概率之和 = 1 的h(x)


​	*梯度下降  几种策略&数学推导：

​		·批量梯度下降	容易得到最优解，每次考虑所有样本，速度慢

​		·随机梯度下降	速度快，但不一定每次都朝着收敛方向

​		·小批量梯度下降	最实用，每次更新选择一小部分数据计算


------


### 2.决策树


​	0.概述：从根节点 (第一个选择点) 一步步（经过非叶子节点与分支）走到叶子结点（最终决策），所有数据最终落在叶子结点，可做分类 & 回归。

​		·如何通过给定数据集构造（训练）决策树？

​			由于每个（分支）节点 是针对样本的某个特征的数据评判

​			首先选取【分类效果最好的】特征 作为根节点，然后对【各个特征对于样本集的分类效果的影响度】按降序，依次选取子级节点

​		熵 （entropy）：

​			随机变量的不确定性的度量（原热力学参量，表示体系混乱程度的度量；通俗理解，即事物内部的混乱程度）

​			假定样本集合D中第k类样本所占比例为pk（pk，即D中类别k出现的概率）

​			则D的信息熵：	Ent( D ) = - Σ[pk * log₂(pk)]  , 其中k = 1,2,...,n		【0.1】

​			=》熵值越大越混乱

​		信息增益：

​			样本集合D以某特征A划分数据集前后的熵的差值

​			(特征A的)信息增益 =  entropy(按A划分前) -  entropy(按A划分后)

​			Gain(D, A) = Ent( D ) - Ent(D | A)	【0.2】

​			=》当划分前的熵值 > 划分后的熵值时，信息增益 为正数

​			=》ID3算法  便是以 信息增益 为衡量标准来构造决策树

​		信息增益率：

​			思考：基于信息增益的 ID3算法，当D的特征集中含有某种与类别完全无关的特征，而且每个样本的属性值都不相同（比如样本的序号id）

​			此时在计算信息增益时，必然得到 该特征 在D中的信息增益最大，而将其作为根节点  =》 为了解决这个问题，引入了概念 【信息增益率】

​			(特征A的)信息增益率 = (特征A的)信息增益 / (按A划分后的)熵值

​			Gain_ratio(D, A) = Gain(D , A) / Ent(D | A)	【0.3】

​			相当于，在 信息增益 的基础上， 乘以一个 惩罚参数。 （特征数量越多，惩罚越小）

​			其中：惩罚参数 = 1 / Ent(D | A)

​			=》C4.5算法 便是以 信息增益率 为衡量标准来构造决策树

​		Gini系数：（基尼系数）

​			从数据集D中随机抽取两个样本，两个样本的类别不同的概率 （通俗理解，其意义类似信息熵，反映样本集的混乱程度）

​			Gini( D ) = 1 - Σ[(pk)²]  , 其中k = 1,2,...,n		【0.4】

​			=》CART算法 便是以 Gini系数 为衡量标准来构造决策树

​		连续值离散化：

​			如果数据集D中含有连续性数值的特征时，使用 数值范围分割， 转化成离散量

​		剪枝：

​			思考：因为决策树算法过拟合的风险很大，试想如果决策树结构足够庞大复杂，每个叶子结点上数据很少，甚至只有一个。

​			剪枝目的：防止过拟合，简化决策树结构，使决策树能够更高效地完成分类任务

​			剪枝策略：

​				预剪枝，(更实用）在建立决策树的过程中进行剪枝；

​					通过 限制决策树深度;  限制叶子结点总数;  限制叶子结点的样本数;  限制信息增益 量

​				后剪枝，在决策树建立之后进行剪枝

​					通过 衡量标准 Cα( T ) = C( T ) + α * |T.leaf|，在每个分支节点处计算，通过比较决定是否保留该节点的分支

​	1.决策树：

​		ID3算法 -》信息增益

​		C4.5算法 -》信息增益率

​		CART算法 -》 Gini系数


------


### 3.贝叶斯


​	0.关于【贝叶斯定理】：

​		逆向概率：

​			常见的例举问题，（如：摸球概率实验）已知了黑白球的数量或球总数，求 概率|比例 问题

​			然而现实情况下，往往事先不知道袋子中 各色球 的具体数量， 需要通过摸球实验，观察取球的实验数据，来推断袋子中 各色球的比例。

​		·为什么用贝叶斯？

​			现实世界本身是不确定的、人类的观察能力有限、"观察"本身只是事物表面的结果不一定反映本质（需要根据表面结果推测本质）

​		贝叶斯公式：(条件概率 P(A|B) = 在B出现的情况下，A发生的概率)

​			P(A|B) = [P(B|A)*P(A)] / P(B)


​	先验概率：

​		根据以往经验和分析得到的概率（往往通过对 历史观测数据 或者 外部数据集/库 的分析求得某事件的先验概率）

​	模型比较理论：

​		最大似然：最符合 (历史) 观测数据的（P(D|h) 最大的）最有优势  【即 当想要输入h(某个猜测值/正确拼写值)时，将其误输入为D的概率 最大】

​		奥卡姆剃刀：P( h ) 最大的 有较大优势  【即 平时情况下输入h的概率 最大】

​			=》平面上N个点，可以拟合为一阶的 直线，可以拟合为二阶的 曲线，...可以拟合为N-1阶多项式模型，选取那一个模型呢？

​			=》根据 奥卡姆剃刀原则，越是高阶的多项式模型越不常见【即 P(h) 较小】，所以倾向于拟合为更低阶、更简单的模型


1.朴素贝叶斯：假设特征之间的条件独立性 （P(ABC) =概率*公式=》 P(A)*P(B| A)*P(C| AB) =贝叶斯假设=》 P(A)*P(B)*P(C)）


​	垃圾邮件过滤问题：

​		给定一封邮件 D，判断它是否为 垃圾邮件， 邮件D由n个单词（d1，d2，...，dn）组成，h+表示垃圾邮件，h-表示正常邮件

​		是垃圾邮件的概率：	P( h+ | D) =   [P(h+) * P(D|h+)] / P(D)

​		不是垃圾邮件的概率：	P( h - | D) =   [P(h -) * P(D|h -)] / P(D)

​		=》P(h+-) 先验概率可根据历史数据集求得

​		=》P(D|h+) 可拓展为=》 P(d1,d2,...,dn | h+) = P(d1| h+)*P(d2| d1,h+)*P(d3| d1,d2,h+)*...*P(dn| d1,d2,...,dn-1,h+)	【1.1】

​			·朴素贝叶斯 假设 特征之间是独立，互不影响的 

​			=》 【1.1】中，di 与 di-1 是完全无关的，于是【1.1】可简化为 P(D|h+) = P(d1| h+) * P(d2| h+) *...* P(dn| h+)

​			而对于 P(di| h+) ，只需要 统计 di （这个单词）在 h+ (垃圾邮件) 中出现的概率 即可

​		P(h+-)、P(D|h+)都已求得，P(D)都相同可约分，于是可以求得 当前邮件 是否为 垃圾邮件的概率

​		=》实际情况下，di 与 di-1之间的某种相关性，必定对整个邮件的判断有影响，但是为了方便问题的求解，可忽略该影响使用 朴素贝叶斯方法 来解决


​	单词拼写纠正问题：（ 对于误输入 tha =》 the ？ than ？=》 真正想输入的单词是什么？）

​		=》求条件概率： P（ 猜测用户要输入的单词 | 用户实际输入的单词 ）

​		假设 用户实际输入单词 D （观测数据），猜测集有 h1 、h2 、h3...  相应的概率为 (p(h1|D))、(p(h2|D))、(p(h3|D))...	

​		=》统一为 P( hi|D )，由贝叶斯公式有： P( hi|D ) = [P( hi ) * P( D|hi )] / P( D ) 	【0.1】

​		由于对所有 hi，P( D ) 都是一样的， 所以对于输入为D时，各个猜测情况的概率有 (正比) 关系： P( hi|D ) ∝ P( hi ) * P( D|hi )


#### 贝叶斯系列：


#### HMM隐马尔可夫模型：


#### MRF马尔科夫随机场：


#### CRF条件随机场：


#### LDA主题模型：


------


### 4.支持向量机


​	0.概述：SVM(support vector machine)算法原理：

​		将含两种类别的样本空间通过一个决策边界分开，使得离决策边界最近的样本点能够尽可能远离决策边界 (通俗理解：找到一条线，使得离该线最近的点能够最远)	

​	1.点面距离公式 =》svm的中最优解问题：

​	===》几何公式：

​		点（x0，y0）到直线 Ax + By + C = 0 的距离 d = |Ax0 + By0 +C| / ||A² + B²||		【0.1】

​	===》样本空间下描述:（含x轴y轴的二维平面，等价于 含两种特征的样本空间）

​		在含两种特征f1,f2的样本空间中，某个样本点Xi(特征f1 = x1,特征f2 = x2) 与某个特征划分边界D: w1f1 +w2f2 + b = 0 的(欧氏)距离为：

​		d(Xi,D) = |w1x1 + w2x2 + b| / ||w1² + w2²||	【0.2】

​		将特征划分边界D: w1f1 + w2f2 + b = 0 写成向量形式=》WtF + b = 0 

​		则点面距离可写为: d(Xi,D) = |WtXi + b| / ||W|| ,其中W=(w1,w2),F=(f1,f2),Xi=(x1,x2)		【0.3】

​	===》样本空间  推广至N维：

​		N维特征(f1,f2,...,fn)空间有( 设样本点Xi'=(x1,x2,...,xn),特征划分平面D': w1'f1'+w2'f2'+...+wn'fn' +  b' = 0 即 W'tF + b' = 0)： 

​		d(Xi',D') = |W'tXi' + b| / ||W'||		【0.4】

​	决策方程的变换：设样本的类别Y：X为正例时 y = +1，负例时 y = -1，若决策方程为 y(x) = WtΦ(x) + b （令决策边界为WtΦ(x) + b = 0, Φ为对x执行某种变换）则有：

​		决策边界的某一侧有正例点xi ： y(xi) = WtΦ(xi) + b > 0 ,   正例，xi对应标记  yi = +1 > 0

​		则决策边界另一侧的负例点xi'：y(xi') = WtΦ(xi')+ b < 0 ,   负例，xi'对应标记 yi'= - 1 < 0

​		于是， 令 yi * y(xi) 必定大于0 (根据这个性质，消去 距离公式中的绝对值符号 | ? |)

​		经过化简得到SVM中的 样本·决策边界 "距离"为：（消去绝对值符号，下式的D不是严格意义上的 距离d）

​			D = yi * d = yi * (WtΦ(xi) + b) / ||W||		【0.5】

​	初提目标函数：argmax_W,b { min_i[ D ] }  =  argmax_W,b { min_i[ yi * (WtΦ(xi) + b) / ||W||]  

​		===>  argmax_W,b { (1 / ||W||)*min_i[ yi * (WtΦ(xi) + b)] }		【0.6】

​	放缩简化：通过对决策方程y(x; W,b)的放缩使其总是满足  yi * (WtΦ(xi) + b) >=1 		【0.7】

​		于是 min_i[ yi * (WtΦ(xi) + b)] = 1 ,【0.6】简化为：argmax_W,b { 1 / ||W|| }	【0.8】

​	max_W,b {1/||W||}  =最值转换=》  min_W,b{||W||}  =问题转换=》 min_W,b{ (1/2)*W² }	【0.9】，约束条件【0.7】不变


2.拉格朗日乘数法：（常用于求解最优化问题，按条件分为三种：无约束 & 等式约束 & 不等式约束）

	1.无约束：求目标函数对各个自变量？的导数，求导函数等于0的点可能是极值点，结果回代验证

	2.等式约束:

		求n元函数ƒ(x1,x2,…,xn)在 m个条件函数 φj(x1,x2,…,xn)=0, 其中j=1,2,…,m 下的条件极值:

		拉格朗日函数 L(x1,x2,…,xn, λ1,λ2,...,λm)=ƒ(x1,x2,…,xn)+ ∑_m{ λjφj(x1,x2,…,xn)} ,求 L 关于x1,…xn的偏导数，令它们等于零并与m个条件函数联立φ，则

			L'xi=ƒ'xi+ ∑_m{ λjφ'j }=0 ,其中i=1,2,…,n.j=1,2,…,m	【0.10】

			φj(x1,x2,…,xn)=0 ,其中j=1,2,…,m		【0.11】

		联立【0.10】& 【0.11】解得 x1,x2,…,xn 为ƒ可能极值点

	3.（含）不等式约束：

		求n元函数ƒ(x1,x2,…,xn)在 p个等式条件函数 hj(x1,x2,…,xn)=0 和q个不等式条件函数 gk(x1,x2,…,xn)<=0 ,其中j=1~p,k=1~q 下的条件极值:

		拉格朗日函数 L(x1,x2,…,xn, λ1,λ2,...,λp, μ1,μ2,...,μq) = ƒ(x1,x2,…,xn) + ∑_p{ λjhj(x1,x2,…,xn)} + ∑_q{ μkgk(x1,x2,…,xn)}

		KKT条件 -> 最优值必须满足的条件：L'xi = 0; hj(x1,x2,…,xn)=0; θ*gk(x1,x2,…,xn)=0 ，联立求得可能的最优解。

		=》注意到 第三个KKT条件 θ*gk(x1,x2,…,xn)=0, gk <= 0. 若 gk != 0时必有 θ=0


3.SVM中的 拉格朗日乘数法推导：

	对于最优化问题：min_W,b{ (1/2)*W² }，(不等式)约束函数(参考【0.7】)：yi * (WtΦ(xi) + b) >=1

	求n+1元函数 ƒ(W,b) = (1/2)*W²在 n个不等式条件函数  gi(W,b) = 1 - yi * (WtΦ(xi) + b) <= 0 ,其中i=1~n下的条件极值：

	拉格朗日函数 L(W,b,  λ1,λ2,...λn) = ƒ(W,b) + ∑_n{ λigi(x1,x2,…,xn)} 

	=》 L(W,b,λ) = (1/2)*||W||² - ∑_n{ λi[ yi(WtΦ(xi) + b) - 1 ]} 		【0.12】

	KKT条件 & 对偶性质： min_W,b{ max_λ{ L(W,b,λ) } }   -->   max_λ{ min_W,b{ L(W,b,λ) } }

	·L对W求偏导：əL/əW = 0	=》	w = ∑_n{ λiyiΦ(xn) }		【0.13】

	·L对b求偏导：  əL/əb = 0	=》	0  = ∑_n{ λiyi }		【0.14】

	【0.12】展开得： L = (1/2)*WtW - Wt*∑_n{ λiyiΦ(xi) } - b*∑_n{ λiyi } + ∑_n{ λi }	【0.15】

	将【0.13】与【0.14】代回到【0.12】得：L = ∑_n{ λi } - (1/2)*∑_n_ij{ λiλj*yiyj * Φt(xi)Φ(xj) }	【0.16】至此完成min_W,b{L}求解任务

	继续求 max_λ{ L } ，将【0.16】代入得：max_λ{ ∑_n{ λi } - (1/2)*∑_n_ij{ λiλj*yiyj * Φt(xi)Φ(xj) } }		【0.17】

	=》由第三个KKT条件有： ∑_n{ λiyi } = 0 ,由拉格朗日乘数法条件：λi >=0; 

	【0.17】极大值问题转换成极小值问题：min_λ{ (1/2)*∑_n_ij{ λiλj*yiyj * Φt(xi)Φ(xj) - ∑_n{ λi } } ,约束条件 ∑_n{ λiyi } = 0，λi >=0	【0.18】

	

4.SVM中的 Soft-margin 软间隔：

	如果严格按svm的要求来确定决策边界，有时候数据中的极少数噪点，会对 决策边界对样本的划分效果 (svm的分类效果) 产生较大影响

	因此，可以适当放松确定决策边界的要求，如通过引入 【松弛因子ξi】有： yi * (Wt·xi + b) >= 1 - ξi		【0.19】(基于【0.7】引入ξi)

	令 Δ = yi * (Wt·xi + b) -1 >= - ξi  ,可以写作 Δ_min = - ξi	【0.20】

	将【0.19】代入拉格朗日函数【0.12】(目标函数)：  min{ (1/2)*||W||² - ∑_n{ λi[ yi(WtΦ(xi) + b) - 1 ] }	

	=》 min{ (1/2)*||W||² - ∑_n{ λi * Δ ] }  =》min{ (1/2)*||W||² + ∑_n{ λi * ξi ] }  =》	

	设C为常数，新目标函数： min{ (1/2)*||W||² + ∑_n{ λi * ξi ] } = min{ (1/2)*||W||² + C ∑λi] }		【0.20】

		C 大  ===》 严格按照要求确定决策边界 

		C 小  ===》 允许忽略少量噪声点的干扰 

	常数C 可根据实际样本情况来指定 =》 C越大，间隔越“硬”；C越小，间隔越“软”


5.拉格朗日乘数法 中引入 松弛因子ξi 的推导：

	对【0.12】拉格朗日函数的推导：L(W,b,λ) = L(W,b,λ, ξ) =  (1/2)*||W||² - ∑_n{ λi[ yi(WtΦ(xi) + b) - 1 ]} - C ∑ξi + C ∑ξi

	令 L(W,b,λ, ξ) - ∑μiξi  = L(W,b,λ, ξ,μ) = (1/2)*||W||² - ∑_n{ λi[ yi(WtΦ(xi) + b) - 1 + ξi ] + C ∑ξi - ∑μiξi		【0.21】

	由【0.13】【0.14】有约束条件:	w = ∑_n{ λiyiΦ(xn) } ,0  = ∑_n{ λiyi }	【0.22】

	·L对ξi求偏导： əL/əξi = 0	=》	C - λi - μi = 0 	【0.23】

	·拉格朗日乘数法-松弛因子条件：λi >=0, μi >= 0	【0.24】

	综合约束条件【0.22】【0.23】【0.24】可将原最优解问题转化为：

		min_λ{ (1/2)*∑_n_ij{ λiλj*yiyj * Φt(xi)Φ(xj) - ∑_n{ λi } } ,约束条件 ∑_n{ λiyi } = 0, λi∈[0,C]	【0.25】


6.K 核函数： 决策边界为WtΦ(x) + b = 0 中的Φ(x)（以下写为K）为对x执行某种变换，这种变换的

	目的：将 低维·线性不可分(难分) =转化=》高维·线性可分(易分)

	设x,z∈X, X属于R(n)空间，非线性函数Φ实现输入空间X到特征空间F的映射，其中F属于R(m)，n<<m

	·低维n		原决策方程为 y'(x) = w'1x1 + w'2x2 +...+ w'nxn + b  ,其中 i=1~n表示特征量下标

	·映射到高维m	决策方程为 y(x) = w1K(x,x1) + w2K(x,x2) +...+ wmK(x,xm) + b  ,其中 j=1~m表示样本空间内的m个样本点的下标

		wjK(x,xj) 中的 x和xj 的意义：x是当前待分类的样本点，xj为训练时从样本空间中 手动(指定)选取的m个样本点

	核函数的类型有以下:( j =1~m 为预先选取的m个样本点的 样本下标) 

	1.线性 核函数：(线性可分情况下可使用）	

		K(x,xj) = x·xj	(·点乘、<,>内积、对应项乘积之和)	

	2.多项式 核函数：

		K(x,xj) = [ ν(x·xj) + R ]^d  	（通常取R = 1，d = 2）

	3.高斯 核函数 ( RBF径向基核函数 )：(灵活性高, 应用最广,)

		K(x,xj) = exp(-||x - xj||² / 2σ²)		

	4.Sigmoid 核函数：

		K(x,xj) = tanh[ ν(x·xj) + c ]

7.SMO算法

8.关于如何选取 核函数 的一些建议：

	 0.一般用线性核和高斯核，时间上高斯核耗费较大，要注意需要对数据归一化处理


 1. 如果Feature 多，与样本量相当，选用 线性回归LR 或 线性核svm (Linear Kernel)


 2. 如果Feature 少，而样本量不大不小，选用 高斯核svm (Gaussian Kernel)


 3. 如果Feature 少，而样本量大，可以手动添加feature，参考情况1：线性回归LR 或 线性核svm (Linear Kernel)


    


------


### 5.集成学习 


​	0.概述：集成学习 Ensemble learning：【Bagging、Boosting、Stacking】

​		指将若干弱分类器 (或基(础)分类器) 组合之后产生一个强分类器 （可以是不同类型的分类器）

​		·并不算是一种分类器，而是一种分类器的结合方法；

​		·一个集成分类器的性能会好于单个分类器；

​		


#### 1.Bagging 算法(bootstrap aggregation)：


​	多个分类器同时对一个样本进行分类计算，对它们的结果 取平均 【并行训练，结果取平均值】

​	设共有N个(弱)分类器, 第n个分类器的 分类模型为 fn(x)

​	则这个集成的强分类器的分类模型为  fa(x) = (1/N)Σ[fn(x)]   其中n = 1,2,...,N		【1.1】


​	·随机森林 (Random Forest):

​		Bagging 思想的最典型代表

​		随机： 数据采样 随机（采样量相同，常用0.6~0.8D）， 特征选择 随机 

​			（特征选取量相同，常用0.6~0.8K，K为特征总数）

​		森林： 多个决策树 并行放在一起


#### 2.Boosting 算法(Boostrapping):


​	一种把若干个(弱)分类器 (前后串联) 整合为一个(强)分类器的方法 【串行训练，对前者的预测值相对真实值的残差弥补】

​	对于一个含有 m-1 个弱分类器的 模型fm-1(x)，在添加一个 弱分类器 h(x) 后

​	集成的强分类器的模型为 ：fm(x) = fm-1(x) + argmin_h { Σ[L(yi, fm-1(xi) + h(xi))] } 		【2.1】 

​	【2.1】中函数  argmin_x { f(x) } 是指 使得函数 f(x) 取得其最小值时所有自变量 x 的集合 （f 取最小值时 x 的值）

​	

​	Boosting 

​	|->XGBoost   (Xtreme Gradient Boosting 极限 梯度 增强)

​	|->AdaBoost   (Adaptive Boosting 自适应 增强)	

​	|->GBDT   (Gradient Boosting Decision Tree 梯度提升决策树)

​	|->LightGBM   (Light Gradient Boosting Machine 轻量级 梯度提升机)


#### ·XgBoost   (Xtreme Gradient Boosting 极限 梯度 增强)


​		1.基本描述：

​		假设Xg-模型有 t 颗决策树数，t棵树有序串联构成整个模型，各决策树的叶子节点数为 k1,k2,...,kt,  

​		对于决策树 Ti, 叶子节点数为 ki, 设这颗数每个叶子节点上的权值为：wj_i  (i∈[0,t]为决策树下标，j∈[0,ki]为叶子节点下标)

​		该模型在 对一个样本进行 分类/回归 时，这个样本数据点 在树的根节点输入, 在树的某一叶子节点输出


​		2.模型构造：

​		=》Xg-模型有 t 颗决策树数，假设该模型由0颗数 逐一新增至 t颗树的 数学模型变化过程

​		当	t = 0 时:		y_0 = F0(x) = f0(x) = 0

​			t = 1 时:		y_1 = F1(x) = F0(x) + f1(x) = f1(x)

​			t = 2 时:		y_2 = F2(x) = F1(x) + f2(x) = f1(x) + f2(x)

​			  ...		      ...

​			t = t 时:		y_t  = Ft(x) = Ft-1(x) + ft(x)		

​		其中当t = i时，即Xg-模型 为 i颗决策树构成时:

​			fi(x)	为第i颗 (最后一棵) 决策树的 数学模型 (函数)

​	 		y_i 	为样本数据x 经过整个xg模型的分类输出值

​			Fi(x) 	为整个Xgboost的  数学模型 (函数)


​		在 t-1 颗决策树构成的 数学模型为 Ft_1(x) 的Xg-模型上，(末尾)新增一颗 数学模型为ft(x)的决策树

​		新构成的Xg-模型的 数学模型为：y = Ft(x) = Ft-1(x) + ft(x)		【2.2】

​		

​		3.结构分析：

​		第t棵决策树，各个叶子节点的权值为 w1,w2,...,wk，树的数学模型为：ft(x) = wq(x)   

​		wq 表示, 当这棵决策树的输入为x时, 样本被分类到下标为q（q∈[1,k]）的叶子节点上对应的 权值 (向量)

​		若样本数据维数为d，对于样本 X(x_1, x_2, ..., x_d), 权值 wq(wq_1,wq_2,...,wq_d)有

​		ft(x) = wq(x) , w ∈ Rt , q: Rd   →   {1,2,...,T}		【2.3】

​			T: 树的数量,   Rt: 维度为t的向量空间,   Rd:  维度为d的向量空间,   q:表示1到T之间的树下标t下的解叶子结点的下标

​			*向量空间Rn: 设有非空集合V(元素均为n维向量)、域P(向量各维度的值类型,实数->实数域,复数->复数域),向量的加法&数乘运算满足8条件

​		        = (wq_1,wq_2,...,wq_d)t(x_1, x_2, ..., x_d) 

​		        = (wq_1*x_1, wq_2*x_2, ..., wq_d*x_d)t

​			

​		· 定义第t颗构造树的 惩罚函数   Ω(ft) = γL + (1/2)λ*∑L_i{ wi² }		【2.4】

​			其中，γ:惩罚(系数)力度，L:该树的叶子结点数量，wi:这棵树各个叶子结点的权重向量模的平方

​		Xg模型 - 优化目标：在加入第t颗构造树时，所选取一个 ft 使得整体的损失(目标)函数 尽量大地降低


​		· 定义整个Xg模型的目标(损失)函数   Obj(t) = ∑n_i{ lost(yi, Ft(xi)) } + ∑t_j{ Ω(fj) }		【2.5】

​			其中，n为样本量，i为样本下标，xi为第i个样本，yi为第i个样本的真实类别

​			       ，t 为Xg模型构造树的个数，j为树的下标，fj为第j棵树的 惩罚函数

​			       ，lost(yi, Ft(xi))表示 当前Xg模型的预测函数Ft(x) 对于第i个样本xi的预测值，与其真实值yi 所求得的 损失值

​		将【2.2】代入【2.5】=》 

​		Obj(t) = ∑n_i{ lost(yi,  Ft-1(xi) + ft(xi) } + C + Ω(ft)		【2.6】

​			其中C(Constant)为常数 (前t-1颗构造树的惩罚项值的和 & ∑中导出的定值项)

​			设lost(a,b) = (a-b)²,则=》

​		           = ∑n_i{ [yi - (Ft-1(xi) + ft(xi))]² } + C + Ω(ft)		平方项添-号，并展开

​		           = ∑n_i{ [(Ft-1(xi) - yi) + ft(xi)]² } + C + Ω(ft)		

​		           = ∑n_i{ (Ft-1(xi) - yi)² + ft(xi)² + 2*(yi - Ft-1(xi))*ft(xi) } + C + Ω(ft)	定值项 Ft-1(xi) - yi，导出到C

​		           = ∑n_i{ ft(xi)² + 2*(Ft-1(xi) - yi)*ft(xi) } + C + Ω(ft)		【2.7】残差项: yi - Ft-1(xi)	

​				

​		用·泰勒展开式·来近似原目标函数：

​			·泰勒展开式：若f(x)二次可导，则 f(x + △x) ≈ f(x) + (1/1!)f'(x)△x + (1/2!)f''(x)△x²		【2.8】

​			因为 lost(yi,  Ft-1(xi) + ft(xi)) = [yi - (Ft-1(xi) + ft(xi))]²		其中 lost(a,b) = (a-b)²

​					              = [(Ft-1(xi) - yi) + ft(xi)]²	

​						令 x=(Ft-1(xi) - yi), △x=ft(xi), 设函数 Lost(x) = x²		【2.9】

​					              = Lost(x + △x)

​			≈ Lost(x) + Lost'(x)△x + (1/2)Lost''(x)△x²		泰勒展开	【2.10】

​				其中，Lost(x) = (Ft-1(xi) - yi)²

​				Lost'(x) = 2(Ft-1(xi) - yi)*Ft-1'x	 		= gi

​				Lost''(x)= 2*[Ft-1'x² + Ft-1''x*(Ft-1(xi) - yi)] 	= hi

​			将【2.8】和 gi、hi全部代入【2.10】得：

​			lost(yi,  Ft-1(xi) + ft(xi)) ≈ (Ft-1(xi) - yi)² + gi*ft(xi) + (1/2)hi*ft²(xi)		【2.11】

​					       = (yi - Ft-1(xi))² + gi*ft(xi) + (1/2)hi*ft²(xi)

​					       = lost(yi,  Ft-1(xi)) + gi*ft(xi) + (1/2)hi*ft²(xi)		【2.12】

​		将【2.12】代入【2.6】得：

​			Obj(t) = ∑n_i{ lost(yi,  Ft-1(xi) + ft(xi) } + Ω(ft) + C

​			           ≈ ∑n_i{ lost(yi,  Ft-1(xi)) + gi*ft(xi) + (1/2)hi*ft²(xi) } + Ω(ft) + C

​			           = ∑n_i{ gi*ft(xi) + (1/2)hi*ft²(xi) } + Ω(ft) + C				定值项 lost(yi,  Ft-1(xi))，导出到C

​			           = ∑n_i{ gi*ft(xi) + (1/2)hi*ft²(xi) } + γL + (1/2)λ*∑L_j{ wj² } + C		将【2.4】代入

​			           = ∑n_i{ gi*wq(xi) + (1/2)hi*wq²(xi) } + γL + (1/2)λ*∑L_j{ wj² } + C		将【2.3】代入

​		为了化简(整)表达式，需要将 样本遍历:n_i 和 当前构造树的叶子结点的遍历:L_j 统一为 【叶子结点的遍历 L_j】

​		分析：对于新添构造树，有 n 个样本输入,每一个样本点终必会分类到该树 L 个叶子节点的某一个 节点上, 定义 I 为 i (i=1,2,...,n,表示n个样本的下标) 的集合

​		设n个样本经过新添构造树分类后, 其L个叶子结点上样本子集分别为 I1,I2,...IL, 则有集合关系 I1+I2+...+Ij+...+I L = N{1,2,...,n}, 

​		j 为L个叶子结点下标, Ij 表示第 j 个叶子结点上 分布的 子样本集的 下标集

​			           = ∑L_j{ ∑Ij_i{ giwj } + (1/2)∑Ij_i{ hiwj² }  + (1/2)λwj² }} + γL + C	

​			           = ∑L_j{ ∑Ij_i{ giwj } + (1/2)∑Ij_i{ hiwj² }  + (1/2)λwj² }} + γL + C

​			           = ∑L_j{ wj*∑Ij_i{ gi } + (1/2)(wj²)*∑Ij_i{ hi + λ }} + γL + C			【2.13】

​		令 ∑Ij_i{ gi } = Gj, ∑Ij_i{ hi + λ } = Hj 代入【2.13】则=》(注意在 ∑L_j{∑Ij_i{ * }} 中若将∑Ij_i{ * }视为整体,内部下标 i被消化, 外部下标 j还存在)

​			           = ∑L_j{ wj*Gj + (1/2)(wj²)*(Hj + λ)} + γL + C				【2.14】


​		由于求解目标为 当新添树的各个叶子结点上的 w 为何值时 (wj=?), 能够使得 整体的 损失函数值 (Obj(t)min) 取得最小, 则

​		由于目标 w 存在于 新添树的数学模型 ft 中, 令 J(ft) = Obj(t) 

​			əJ(ft) / əwj =令= 0 

​			                   = əObj(t) / əwj		【2.14】式对 wj 求导

​			                   = ∑L_j{ Gj + wj*(Hj + λ)} = 0

​				  =》 Gj + wj*(Hj + λ) = 0

​				  =》 wj = - Gj / (Hj + λ)		【2.15】

​		将【2.15】代入到【2.14】得：

​			Obj(t) = -(1/2)∑L_j{ Gj² / (Hj + λ) } + γL + C	【2.16】

​		因为目标是使函数值越小越好，可忽略常数C，则目标函数为：Obj(t) = -(1/2)∑L_j{ Gj² / (Hj + λ) } + γL		【2.17】

​			要使 Obj(t) 函数值 越小越好，即 V = (1/2) Gj² / (Hj + λ) 越大越好, 将 V 记为 模型增益分数		【2.18】

​		

​		·在明确了 新添第t颗构造树 有L个叶子结点 和 各叶子结点的权值优化目标 基础上, 通过下面的方法来确定 该树的划分结构

​			由于每一个确定的样本 xi, 对应确定了 gi,hi

​			假设将 n 个样本xi 按某一指标 如x<a划分为两份, 归为 左子树部分 I_left 和 右子树部分 I_right，则

​			其中 I_left、I_right 分别为归为左/右子树部分的 样本的下标 i 的集合，则

​				Gl =  ∑I_left_i{ gi }		Hl = ∑I_left_i{ hi + λ } 

​				Gr =  ∑I_right_i{ gi }		Hr = ∑I_right_i{ hi + λ }


​		定义若当前所选取的 划分方案为Strategy ,则该方案的 模型增益分数 可做如下定义： 

​		VGain = 【划分后的模型增益分数 - 划分前的模型增益分数】- 新增叶子结点带来的复杂度代价(增益阈值)

​		           = (1/2)[ Gl²/(Hl + λ) + Gr² / (Hr + λ) - (Gl+Gr)² / (Hl+Hr+λ)] - γ		【2.18】

​		目标是使【2.18】函数值 越大越好，其中增益阈值γ


​		·枚举 各种划分方法构成的各种树结构，计算该结构的 模型增益分数，选取 增益分数 最大的划分方案

​		由于树的结构有很多可能，所以对于 精确搜索的情况，可采用贪心算法

​		·贪心算法：贪婪地增加树的叶子结点数目，1.对于每个叶子结点尝试增加一个分裂点，2.对于每一次分裂穷举所有可能的分割方案

​		如何穷举 所有可能的分割方案？

​			·对样本X的每一个特征值(x_1, x_2, ..., x_d) 分别进行 实例的排序

​			·用线性扫描 寻找该特征的 最优分裂点

​			·对所有特征选取一个 最佳分裂点

​		优化终止条件：当可选优化方案中 最大增益分数 低于阈值时；当叶子节点数达到上限时；...


#### 	·AdaBoost 自适应 增强


​		Boosting系列代表算法，对同一训练集训练出不同的(弱)分类器，然后集合这些弱分类器构成一个更优性能的(强)分类器

​		传统Boosting方法存在两个问题：


​			1.如何调整训练集的权重分布以训练出不同的弱分类器


​			2.如何将各个弱分类器联合起来组成一个强分类器

​		AdaBoost解决方案：

​			1.从均匀权重分布开始，后一轮弱分类器的输入权重分布将

​				·提高前一轮弱分类器分类错误样本的权重

​				·降低前一轮弱分类器分类正确样本的权重

​			2.计算各个弱分类器 在训练集上的分类误差率，根据误差率计算各个弱分类器的 加权系数

​				·对新样本进行分类时，将各个弱分类器的计算结果加权求和，求得最终的分类结果

​		假设训练一个含有m个弱分类器Gi(i=1,2,...,m)的AdaBoost模型，n个训练样本xj,yj(j=1,2,...n)，m个弱分类器输入权重分布 分别为 

​		第一个分类器：D1(w11, w12, ..., w1j, ..., w1n)

​		第一个分类器：D2(w21, w22, ..., w2j, ..., w2n)

​	     	    ....       ：...

​		第一个分类器：Dm(wm1,wm2,...,wmj,...,wmn)	

​		【注意：任何情况下权重之和都为1，即对Di有 ∑n_j{wij} = 1】

​		1.初始化 D1中 w1j = 1/n, 按样本集以D1的权重分布 来训练 分类器 G1，得到分类模型 G1(x)

​			·计算 G1(x) 在训练集上的 分类误差率 e:		(被G(x)误分类样本的权值之和)

​				e1 = P( G1(xj)≠yi )

​				     = ∑n_j{ w1j*  I(G1(xj)≠yi)}		【2.19】

​				其中 I(条件表达式) = 满足条件时值为1，不满足条件时为0

​			·计算 G1(x) 的加权系数 α：			(多分类器集成时使用的权值)

​				α1 = 1/2 * ln( (1-e1)/e1 )		【2.20】

​			·计算 G1(x) 的泛化因子 Z1:

​				Z1 = ∑n_j{ w1j*exp( -α1*yj*G1(xj) ) }	【2.21】  

​		2.更新第二个分类器的权值分布 D2(w21, w22, ..., w2j, ..., w2n)

​				w2j = (w1j / Z1) * exp( -α1*yj*G1(xj) )		【2.22】

​			·按样本集以D2的权重分布 来训练 分类器 G2，得到分类模型 G2(x)

​			·计算 G2(x) 在训练集上的 分类误差率 e2

​			·计算 G2(x) 的加权系数 α2

​			·计算 G1(x) 的泛化因子 Z2

​		3.同步骤2，依次迭代到 第m个分类器 为止

​			至此，得到m个分类器的 分类模型 G1,G2,...,GM 及其加权系数 α1,α2,...,αm

​		4.构建m个弱分类器的 线性组合模型 f(x)：	

​			f(x) = ∑m_i{ αi * Gi(x) }			【2.23】

​		=》得到最终的AdaBoost分类模型 Cls(x) = sign(f(x)) = sign( ∑m_i{ αi * Gi(x) } )		【2.24】


#### 	·GBDT 梯度提升决策树


#### 3.Stacking：堆叠模型（暴力性的拿一堆分类器来使用，分类器可以是 KNN、SVM、RF 等等）


​	对一堆分类器分别训练，训练完成后对某一个新样本 同时进行分类，各个分类器可能有不同的分类结果...

​	假设有 k 个分类器，样本的某一种类别(标记) yi ∈ {Y1,Y2,...,Yn}，则将产生 k 个分类结果 y1,y2,...,yk

​	最后，构造一个分类器 （如 LR，Logistic回归分类器），将这k个 分类结果y 组成一个 向量(y1,y2,...,yk)作为这个分类器的 输入，输出最终的 分类结果

​	=》堆叠算法准确率很高，但是速度及运算效应是个问题


### 6.聚类  (七大算法)


​	0.概述：聚类属于无监督问题，即对没有标记的样本集中相似的样本聚为一类

​		·聚类难点问题：

​			如何评估模型好坏？

​			如何调节参数优化？

​		· 聚类评估标准

​			轮廓系数 Silhouette Coefficient


#### 1.K-Means算法：


​	·簇: 	指定一个 K 值表示要将 数据集 聚为几个簇 C1,C2,...Ck (类别)

​	·质心：	通过计算每个 簇 内所有样本的所有特征的均值 得到簇的质心, Ci 的质心点为 ci

​	·距离度量:	先将各个特征数据 标准化 (将特征的不同尺度的数值标准化到同一尺度), 然后计算两个样本点xa,xb的 欧氏距离 dist(ax,ab)

​	·优化目标:	min∑k_i∑x∈Ci{ dist(ci,x)² }

​	设有n个样本 x1,x2,...,xj,...xn, 则算法流程:

​		1.设定一个 k 值，在样本空间中，随机初始化(生成) k个点 作为 初始质心c01,c02,...,c0k 

​		2.对于每一个样本点 xj (j=1,2,...,n) 分别计算其到 k个初始质心 的距离 dist_j1(xj, c01), dist_j2(xj, c02), ..., dist_jk(xj, c0k)

​			若 k个距离值中 最小值为 dist_ji, 则将这个样本点 暂时归为 簇Ci

​		3.所有样本点都被归为了 某一个簇，计算各个簇内所有样本点的质心 c1,c2,...,ck (特征均值点)

​		4.重新计算 每一个样本点 xj (j=1,2,...,n) 到k个质心的距离 dist_j1(xj, c1), dist_j2(xj, c2), ..., dist_jk(xj, ck)

​			若 k个距离值中 最小值为 dist_ji', 则将这个样本点 重新归为 簇Ci'

​		5.重复3,4步骤，重新计算各簇质心，重新计算所有样本点最小矩离质心点并归簇，...，直到 达到优化目标 min∑dist

​	优点:	原理简单，速度快，适合常规数据集

​	缺点:	K值往往不确定，算法复杂度 与 样本量相关，由于以均值来计算质心难以发现特殊形状的簇 (如环形簇、弓形簇)


#### 2.DBSCAN算法:


​	Density-Based Spatial Clustering of Applications with Noise 基于密度的具有噪声的聚类方法

​	

​	设定密度取得 半径r，点p在密度核心点q的r邻域内, 则 p-q 直接密度可达

​	若有点的序列 q0,q1,...,1k, 对任意 i-1 & i 的下标点都是 直接密度可达的, 则 q0-qk 密度可达

​	·【密度可达】是【直接密度可达】的传播

​	· q0 与 qk是密度相连的

​	· 边界点：不能再发展外围的非核心点

​	· 离群点：被孤立,不能与任何点(直接)密度可达


​	工作流程：

​		1.输入数据集参数 D

​		2.设置参数-指定 密度半径 r

​			半径r 可根据K距离来设定：找突变点

​			K距离：给定数据集P={p(i); i=0,1,...,n},  计算点Pi到数据集中所有点之间的距离Di（i=0,1,...,n）

​				的距离按从小到大的顺序排序,d(k)就被称为k-距离

​		3.密度阈值 MinPts （一个簇最少包含几个点，否则视为离群点）

​			k-距离 中k的值，一般取小一些，多次尝试

​	

​	优点：	不需要指定簇的个数，可以发现任意形状的簇，擅长找出离群点

​	缺点：	参数难以选择，高维数据直接聚类有困难，算法效率慢


#### 3.EM算法-GMM混合高斯模型：


​	问题：为何会收敛？GMM模型 解释 EM如何工作？

​	【想法】：当一个有单个维度的数据集，可以认为所有数据点都在一个高斯（正态）分布模型下产生，但通常情况下数据点有多个维度的特征，每个维度的特征值可能隶属不同的高斯模型中。【note】不对，这是一个样本空间维度，并不需要考虑样本内部特征，样本隶属不同的分布可能因为采样不同，数据本身的来源不同


​	后验概率 正比于 似然 * 先验

​	P(θ|X) ∝ P(X|θ)·P(θ)

​		

​	琴生不等式、海森矩阵、最大似然估计


#### 4.Hierarchical 层次聚类：


​	计算每一个类别的数据点与所有数据点之间的距离来衡量它们之间的相似性，距离越小相似度越高。将距离最近的两个数据点或类别进行组合，一层一层迭代向上生成聚类树。

​	

​	每次迭代，对全体数据集中的所有

​		·已被聚的簇的质心、未被聚簇的样本点

​	中的距离最近的两个质心聚一个新簇，并求出新簇的质心；

​	


#### 5.MeanShift 均值漂移：


​	Mean shift 算法是基于核密度估计的爬山算法，可用于聚类、图像分割、跟踪等。

​	给定d维空间的n个数据点集X，那么对于空间中的任意点x的 mean shift vector **漂移向量** 可表示为：

$$

M_h=\frac{1}{K}\sum_{x_i\in S_k}{\left( x_i-x \right)}

$$

​	Sk表示数据集的点到x的距离小于球半径h的数据点，即：

$$

S_h\left( x \right) =\left\{ y:\left( y-x_i \right) ^T\left( y-x_i \right) <h^2 \right\}

$$

​	漂移过程 即通过计算得到的漂移向量，把球圆心x的位置更新为 x:=x+Mh，使得圆心一直处于力的平衡位置。


​	图像聚类：

​		一个图像就是个矩阵，如果像素点均匀的分布在图像上，没有稠密性。

​	所以定义 像素点x的 **概率密度** 计算规则：（x为圆心，h为半径，簇内的样本点 xi）

​			· x点与xi点的  颜色越相近，概率密度越高

​			· x点与xi点的  位置越相近，概率密度越高

​	则概率密度可定义为：

$$

K_{h_s,h_r}\left( x \right) =\frac{C}{h_{s}^{2}h_{r}^{2}}K\left( \lVert \frac{x^s-x_{i}^{s}}{h_s} \rVert ^2 \right) K\left( \lVert \frac{x^r-x_{i}^{r}}{h_r} \rVert ^2 \right) 

$$

​	其中 Ks(·)表示空间密度，Kr(·)表示颜色密度，再根据图像概率密度分布，可实现图像分割（传统分割法）。


#### 6.模糊C均值聚类(fuzzy c-means, FCM)：


​	通过优化目标函数得到每个样本点对所有类中心的隶属度，从而决定样本点的类属以达到自动对样本数据进行分类的目的。


![1555641022937](C:\Users\Tinkle_GW\AppData\Roaming\Typora\typora-user-images\1555641022937.png)


​	假设样本集合为X={x1 ，x2 ，…，xn }，将其分成c 个模糊组，并求每组的聚类中心cj （ j=1，2，…，C） ，使目标函数达到最小。1.固定数量的集群。2.每个群集一个质心。3.每个数据点属于最接近质心对应的簇。

​	集群：即模糊集合，一个点的隶属度可以是0到1之间的任何数字，一个点的所有隶属度数之和必须为1。


#### 7.som 自组织神经网络：




------


### 7.神经网络


​	BP算法

​	CNN卷积神经网络 ===》深度学习…


### 8.关联分析


​	Apriori算法

​	FP-growth算法


### 9.降维&稀疏


​	0.概述：

​		向量：α(x1, x2, x3,...) 其中 α可为一个样本的表示，而xi为样本α的各项特征 

​		内积:（点乘，结果为标量）设有 向量A = (a1,a2,...,an)t,  向量B = (b1,b2,...bn)t

​			则A与B 的内积/点乘为：	

​			A·B = (a1,a2,...,an)t · (b1,b2,...bn)t = a1b1 + a2b2 +...+ anbn		

​			A·B = |A||B|cos(θ),  其中θ为向量A与B之间的夹角

​			其中 |A|cos(θ) 的值几何意义：向量A在向量B的方向上的投影 的长度

​		基 & 基变换 & 坐标变换：

​			基的性质：	·基是正交的	·基之间的 内积/点乘 值为0	·基的方向相互垂直	·基向量之间 线性无关

​			设 R^n 中有两组基 ξ(ξ1,ξ2,...,ξn) 、η(η1,η2,...ηn)，向量α 有表示如下：

​				在 基ξ 中，向量α : (ξ1,ξ2,...,ξn) · (p1,p2,...,pn) = (p1ξ1, p2ξ2, ..., pnξn)t 

​				在 基η 中,  向量α : (η1,η2,...ηn) · (q1,q2,...,qn) = (q1η1, q2η2, ..., qnηn)t

​			其中，P(p1,p2,...,pn) 为向量α 在基 ξ(ξ1,ξ2,...,ξn)下的坐标，同理 Q(q1,q2,...,qn) 为向量α 在基 η(η1,η2,...ηn)下的坐标

​			存在一个 可逆矩阵 Cnxn，使得 (ξ1,ξ2,...,ξn) = (η1,η2,...ηn)C = (η1,η2,...ηn)[c11...cnn] = [ η1·(c*1)t, η2·(c*2)t, ..., ηn·(c*n)t ]	【0.1】

​			【0.1】即 基变换公式 ξ=ηC， 矩阵C 为由 基η(η1,η2,...ηn) 到 基ξ(ξ1,ξ2,...,ξn) 的过渡矩阵

​			=》向量α ：(η1,η2,...ηn) · (q1,q2,...,qn) =同表示=》(ξ1,ξ2,...,ξn) · (p1,p2,...,pn) = (η1,η2,...ηn)C  · (p1,p2,...,pn)

​			=》两种表示法消去(η1,η2,...ηn)可得：(q1,q2,...,qn) = C  · (p1,p2,...,pn) 即 Q = CP 或 P = C`Q	【0.2】	

​			=》【0.2】 即 坐标变换公式


kNN：


LDA 线性判别分析：


​		同时属于 降维学习 & 线性学习，

​		方法思想：对于给定样本集，设法将所有样例投影到一条直线上，使得属于同一类的投影点尽可能的接近（聚集），而不同类的投影点彼此远离。

​		对于新的样本进行分类时，将其投影到这条直线上，根据该样本对应投影点的位置来确定（预测）新样本的所属类别。


稀疏表示 & 字典学习：


#### 	PCA 主成分分析:（Principal Component Analysis）


​	最常用的一种 降维 手段，(基于最大化方差)提取最优价值的信息，为了便于后续建立(分类/回归)模型

​	PCA方法的问题：原始特征数据 通过降维后 失去现实含义 （这一特性可以帮助"加密"敏感特征信息）


​	协方差矩阵：

​		·方差： Var(a) = (1/m)∑(ai - μ)²		（某样本集下）某一种特征的分散程度

​		·协方差 (设均值为0时)： Cov(a,b) = (1/m)∑(aibi)	（某样本集下）某两种特征的的相关性

​			当 Cov = 0 是表示两种特征完全独立，为线性无关的

​		需求：如何选择一个方向 (基)，使得 能保留更多的原始信息 （向量由原基投影到新基时，希望向量表示值尽可能分散）

​		=》寻找一个 一维基，使得 一组向量集 经坐标变换 为这个基上的表示后 方差值 最大

​	

​	降维优化分析：

​		若要将一组 k维向量 降至 t维 t∈(0,k)

​		目标是选择 t个单位正交基，使得原始数据变换到这组基后，字段内部  方差尽可能大，各字段之间的协方差为0


​		设 m是样本量，k是原维数 即原特征数，记 xifj 为样本集中第i个样本 第j维特征()i=1,2,...,m  j=1,2,...,k

​		则样本集表示为：X_mxk (行数x列数 || 高x宽 || 列长x行长)	 

​			X = [(x1f1   x1f2   ...   x1fk)

​			        (x2f1   x2f2   ...   x2fk)

​			          ...        ...     ...     ... 

​			        (xmf1 xmf2  ...   xmfk)]

​			Xt= [(x1f1    x2f1    ...    xmf1)

​			       (x1f2    x2f2    ...    xmf2)

​			       ( ...        ...       ...      ...  )

​			       (x1fk    x2fk    ...    xmfk)]

​		协方差矩阵为：（实对称阵_kxk）

​			(1/m)*XXt = [·] ,		【0.3】

​			矩阵[·]中，对角线上的 (1/m)∑m_i(xifj²) 是样本集X中特征 fj 的方差

​			                 其它元素的 (1/m)∑m_i(xifj * xifj') 是样本集X中 特征fj 与 特征fj' 的协方差

​			实对称矩阵性质：1.实对称阵必然 可相似对角化	2.不同特征值λ对应的特征向量ξ 相互正交

​			根据PCA的目标：方差尽可能大，协方差全部为0 

​			=》 协方差矩阵的 对角化（使得矩阵除对角线外的元素均化为0，对角线元素按大小·上下排列）

​		协方差矩阵 对角化： (若C为实对称阵, 则∃可逆P使P`CP = ∧_kxk, ∃正交Q使QCQt = ∧_kxk)

​			PCPt = ∧ = （λ1

​				           	λ2

​				                 	...

​				                      	λk）

​	PCA降维：原始数据集 矩阵 (X_mxk)  ==k维_降至_t维(0<t<k)==》 数据集 矩阵(X'_mxt) 

​		设 λ1,λ2,...,λk 为X的协方差矩阵C的 (前)K个特征值，对应的特征向量为 ξ1,ξ2,...,ξk

​		取这K个特征值按从大到小排列的 前t个特征值λ1,λ2,...,λt 所对应的特征(列)向量ξ1,ξ2,...,ξt 组成的新矩阵 M_kxt  (行数x列数 || 高x宽 || 列长x行长)

​		则降维结果 数据集：

​			X'_mx5 = X_mxk · M_kx5	【0.4】


#### 	ICA 独立成分分析


#### 	RCA 根本原因分析


#### 	SVD奇异值分解


#### 	QR分解	


#### 因子分析


------


### 10.ML相关补充：


半/无监督学习、过/欠拟合（梯度下降、正则化…）、PAC学习理论-假设空间-泛化/经验误差-霍夫丁不等式-VC维、蒙特卡洛方法


协同过滤 - 推荐算法、知识图谱&语义网络、时序数据分析、word2vec&NLP


规则学习-AQ系列算法、归纳学习-Prism系列算法、生成模型->判别模型、稀疏自编码&RBM限制玻尔兹曼机


隐马尔可夫模型、条件随机场


--》领域延伸【统计学、信息论、拓扑学、群论、图论、博弈论、运筹学…】




### *11.智能计算 & 优化算法：


推理算法（确定性&不确定性）、


搜索策略&路径规划（状态空间&启发式搜索、剪枝、回溯、Dijkstra算法、A*算法）、


进化算法（遗传算法）、


群智算法（粒子群优化、蚁群算法）、


模拟退火算法…




### *12.强化学习


进化·遗传算法


PPO算法


DQN算法-Deep Q-Learning


DDPG算法/NAF算法




### *13.迁移学习


学习算法归纳&分类


TrAdaBoost算法


MTL多任务学习算法




### =====================================  数学&算法导论  ========================================


0.博弈论、拓扑学、运筹学

1.分治策略

2.随机算法

3.排序与查找

4.贪心算法

5.动态规划

6.线性规划

7.NP问题

8.近似算法


